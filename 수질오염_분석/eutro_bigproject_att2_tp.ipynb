{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sApOjE1Xbin"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, BatchNormalization, Activation, Dropout\n",
        "from tensorflow.keras import Model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from math import sqrt\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "filename='wq_data1_norn_re.xlsx'\n",
        "df=pd.read_excel(filename, engine='openpyxl')\n",
        "\n",
        "#수질 입력 자료\n",
        "o=df.to_numpy()\n",
        "in_tot=o[:,3:15]\n",
        "\n",
        "in_size=in_tot.shape\n",
        "print(\"total size=\", in_size)\n",
        "\n",
        "#부영양화 지수 변수:COD\n",
        "out_index=o[:,17]\n",
        "out_indx_size=out_index.shape\n",
        "print(\"total size=\", out_indx_size)\n",
        "\n",
        "codmx=13.7\n",
        "chlmx=178.8\n",
        "tpmx=0.332\n",
        "\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(in_tot, out_index, test_size=0.3)\n",
        "\n",
        "tr_size=xTrain.shape\n",
        "print(\"Train size=\", tr_size)\n",
        "\n",
        "vl_size=xTest.shape\n",
        "print(\"Validation size=\", vl_size)\n",
        "\n",
        "tt_ss=len(in_tot)\n",
        "tr_ss=len(xTrain)\n",
        "fe_ss=len(xTrain[0])\n",
        "vl_ss=len(xTest)\n",
        "\n",
        "\n",
        "x1Train=np.reshape(xTrain, (tr_ss, fe_ss, -1))\n",
        "x1Test=np.reshape(xTest, (vl_ss, fe_ss, -1))\n",
        "print(x1Train.shape)\n",
        "print(x1Test.shape)\n",
        "\n",
        "y1Train=np.reshape(yTrain, (tr_ss))\n",
        "y1Test=np.reshape(yTest,(vl_ss))\n",
        "print(y1Train.shape)\n",
        "print(y1Test.shape)\n",
        "\n",
        "tot=np.reshape(in_tot, (tt_ss,fe_ss,-1))\n",
        "print(tot.shape)\n",
        "\n",
        "xxTrain=tf.convert_to_tensor(x1Train, dtype=tf.float32)\n",
        "yyTrain=tf.convert_to_tensor(y1Train, dtype=tf.float32)\n",
        "xxTest=tf.convert_to_tensor(x1Test, dtype=tf.float32)\n",
        "yyTest=tf.convert_to_tensor(y1Test, dtype=tf.float32)\n",
        "ttot=tf.convert_to_tensor(tot, dtype=tf.float32)\n",
        "\n",
        "max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2, strides=2, padding='valid')\n",
        "\n",
        "\n",
        "def spatial_attention(input_feature, name):\n",
        "    kernel_size = 7\n",
        "    kernel_initializer =tf.keras.initializers.variance_scaling()\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "        avg_pool = tf.reduce_mean(input_feature, axis=[2], keepdims=True)\n",
        "        assert avg_pool.get_shape()[-1] == 1\n",
        "        max_pool = tf.reduce_max(input_feature, axis=[2], keepdims=True)\n",
        "        assert max_pool.get_shape()[-1] == 1\n",
        "        concat = tf.concat([avg_pool, max_pool], 2)\n",
        "        assert concat.get_shape()[-1] == 2\n",
        "\n",
        "        concat = Conv1D(1, kernel_size, padding=\"same\", strides=1)(concat)\n",
        "        concat = tf.keras.layers.LeakyReLU()(concat)\n",
        "        assert concat.get_shape()[-1] == 1\n",
        "        concat = tf.sigmoid(concat, 'sigmoid')\n",
        "\n",
        "    return concat\n",
        "\n",
        "\n",
        "def regression_dilated_cnn(Model_input):\n",
        "    # global xss1\n",
        "    xs1=spatial_attention(Model_input, 's1')\n",
        "    xss1=xs1*Model_input\n",
        "    xx1= keras.layers.PReLU()(xss1)\n",
        "\n",
        "    x = Conv1D(16, 3, padding='same', strides=1)(xx1)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = keras.layers.PReLU()(x)\n",
        "\n",
        "    x = Conv1D(32, 3, padding='same', strides=1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = keras.layers.PReLU()(x)\n",
        "\n",
        "    x = Conv1D(64, 3, padding='same', strides=1)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = keras.layers.PReLU()(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(2)(x)\n",
        "    x = keras.layers.PReLU()(x)\n",
        "    \n",
        "    x = Dense(1)(x)\n",
        "\n",
        "    output = Model(Model_input, x, name='regression_dilated_cnn')\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "model_inputs = keras.Input(shape=(fe_ss, 1))\n",
        "AI_model = regression_dilated_cnn(model_inputs)\n",
        "print(AI_model.summary())\n",
        "opt=keras.optimizers.Adam(learning_rate=0.0001)\n",
        "AI_model.compile(loss='mse', optimizer=opt)\n",
        "AI_model.fit(xxTrain, yyTrain, epochs=3200, batch_size=16)\n",
        "train_rs=AI_model.predict(xxTrain)\n",
        "validation_rs=AI_model.predict(xxTest)\n",
        "tot_rs=AI_model.predict(ttot)\n",
        "\n",
        "xs11=spatial_attention(xxTrain,'s1')\n",
        "xs11=xs11.numpy()\n",
        "xs11=xs11.reshape((354, 12))\n",
        "print(xs11.shape)\n",
        "print(xs11.dtype)\n",
        "\n",
        "## estimated variables\n",
        "tp_tr_re=train_rs[:]*tpmx\n",
        "tp_vl_re=validation_rs[:]*tpmx\n",
        "tp_tot_re=tot_rs[:]*tpmx\n",
        "\n",
        "## Observed variables\n",
        "ob_tp_tr=yTrain[:]*tpmx\n",
        "ob_tp_vl=yTest[:]*tpmx\n",
        "ob_tp_tot=out_index[:]*tpmx\n",
        "\n",
        "# Accuracy\n",
        "\n",
        "rms_tp_tr=sqrt(mean_squared_error(ob_tp_tr,tp_tr_re))\n",
        "rms_tp_vl=sqrt(mean_squared_error(ob_tp_vl,tp_vl_re))\n",
        "rms_tp_tot=sqrt(mean_squared_error(ob_tp_tot,tp_tot_re))\n",
        "\n",
        "\n",
        "def nse(predictions, targets):\n",
        "    out=(1-(np.sum((predictions-targets)**2)/np.sum((targets-np.mean(targets))**2)))\n",
        "    return out\n",
        "\n",
        "nse_tp_tr=nse(tp_tr_re,ob_tp_tr)\n",
        "nse_tp_vl=nse(tp_vl_re,ob_tp_vl)\n",
        "nse_tp_tot=nse(tp_tot_re,ob_tp_tot)\n",
        "\n",
        "r2_tp_tr=r2_score(ob_tp_tr, tp_tr_re)\n",
        "r2_tp_vl=r2_score(ob_tp_vl, tp_vl_re)\n",
        "r2_tp_tot=r2_score(ob_tp_tot, tp_tot_re)\n",
        "\n",
        "tp_acc_tr=np.vstack((rms_tp_tr,nse_tp_tr,r2_tp_tr))\n",
        "tp_acc_vl=np.vstack((rms_tp_vl,nse_tp_vl,r2_tp_vl))\n",
        "tp_acc_tot=np.vstack((rms_tp_tot,nse_tp_tot,r2_tp_tot))\n",
        "\n",
        "\n",
        "dd=pd.DataFrame(ob_tp_tr, columns=['TP_OBS_TR'])\n",
        "dd['TP_TR']=tp_tr_re\n",
        "dd.to_csv('model_tr_result.csv', index=False)\n",
        "\n",
        "\n",
        "dr=pd.DataFrame(ob_tp_vl, columns=['TP_OBS_VL'])\n",
        "dr['TP_VL']=tp_vl_re\n",
        "dr.to_csv('model_vl_result.csv', index=False)\n",
        "\n",
        "dw=pd.DataFrame(ob_tp_tot, columns=['TP_OBS_TOT'])\n",
        "dw['TP_TOT']=tp_tot_re\n",
        "dw.to_csv('model_tot_result.csv', index=False)\n",
        "\n",
        "dq=pd.DataFrame(tp_acc_tr, columns=['TP_ACC_TR'])\n",
        "dq['TP_ACC_VL']=tp_acc_vl\n",
        "dq['TP_ACC_TOT']=tp_acc_tot\n",
        "dq.to_csv('model_accuracy.csv', index=False)\n",
        "\n",
        "dy=pd.DataFrame(xs11)\n",
        "dy.to_csv('weight.csv',index=False)\n"
      ]
    }
  ]
}